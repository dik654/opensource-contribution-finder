#!/usr/bin/env python3
"""
Redlib ì¸ìŠ¤í„´ìŠ¤ HTML íŒŒì‹± í…ŒìŠ¤íŠ¸
GitHub Actionsì—ì„œ ì‹¤í–‰í•˜ì—¬ score/ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸

ì‚¬ìš©ë²•:
  pip install requests beautifulsoup4
  python test_redlib.py
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import sys

# â”€â”€ Redlib ì¸ìŠ¤í„´ìŠ¤ ëª©ë¡ (2025ë…„ í™œì„± ì¸ìŠ¤í„´ìŠ¤) â”€â”€
REDLIB_INSTANCES = [
    "https://safereddit.com",
    "https://redlib.tux.pizza",
    "https://redlib.catsarch.com",
    "https://redlib.privacyredirect.com",
    "https://redlib.r4fo.com",
    "https://reddit.rtrace.io",
    "https://redlib.perennialte.ch",
    "https://red.ngn.tf",
    "https://redlib.4o1x5.dev",
    "https://eu.safereddit.com",
    "https://redlib.thebunny.zone",
    "https://reddit.adminforge.de",
    "https://reddit.nerdvpn.de",
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
}

TEST_SUBREDDIT = "todayilearned"


def test_html_parsing(base_url, subreddit=TEST_SUBREDDIT):
    """Redlib HTML íŒŒì‹± í…ŒìŠ¤íŠ¸ â€” score, ëŒ“ê¸€ ìˆ˜, ì œëª© ì¶”ì¶œ ì‹œë„"""
    url = f"{base_url}/r/{subreddit}/hot"
    print(f"\n{'='*60}")
    print(f"[TEST] {url}")
    print(f"{'='*60}")

    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        print(f"  HTTP {resp.status_code} | {len(resp.text)} bytes")

        if resp.status_code != 200:
            print(f"  âŒ ì ‘ê·¼ ì‹¤íŒ¨ (HTTP {resp.status_code})")
            # ì—ëŸ¬ í˜ì´ì§€ ë‚´ìš© ì¼ë¶€ ì¶œë ¥
            if resp.text:
                soup = BeautifulSoup(resp.text, "html.parser")
                err_text = soup.get_text(strip=True)[:300]
                print(f"  ì—ëŸ¬ ë‚´ìš©: {err_text}")
            return None

        soup = BeautifulSoup(resp.text, "html.parser")

        # â”€â”€ 1ë‹¨ê³„: HTML êµ¬ì¡° íƒìƒ‰ â”€â”€
        print("\n  [êµ¬ì¡° ë¶„ì„]")

        # post ì»¨í…Œì´ë„ˆ í›„ë³´ë“¤
        post_selectors = [
            ("div.post", "div.post"),
            ("div.link", "div.link"),
            ("div#siteTable > div", "siteTable children"),
            ("article", "article"),
            (".thing", ".thing"),
            (".post-container", ".post-container"),
            ("[class*='post']", "class contains 'post'"),
            ("[class*='link']", "class contains 'link'"),
            ("[data-fullname]", "data-fullname attr"),
        ]

        found_posts = None
        found_selector = None

        for selector, label in post_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"  âœ… {label}: {len(elements)}ê°œ ë°œê²¬")
                if len(elements) >= 3 and not found_posts:
                    found_posts = elements
                    found_selector = label
            else:
                print(f"  Â· {label}: ì—†ìŒ")

        if not found_posts:
            # ë²”ìš© íƒìƒ‰: classì— postê°€ í¬í•¨ëœ ëª¨ë“  div
            all_divs = soup.find_all("div")
            post_like = [d for d in all_divs if d.get("class") and
                         any("post" in c.lower() for c in d.get("class", []))]
            if post_like:
                print(f"  ğŸ” ë²”ìš© íƒìƒ‰: post-like div {len(post_like)}ê°œ")
                found_posts = post_like[:10]
                found_selector = "generic post-like"

        # â”€â”€ 2ë‹¨ê³„: ì²« ë²ˆì§¸ í¬ìŠ¤íŠ¸ ìƒì„¸ ë¶„ì„ â”€â”€
        if found_posts:
            print(f"\n  [í¬ìŠ¤íŠ¸ ìƒì„¸ ë¶„ì„] (selector: {found_selector})")
            for i, post in enumerate(found_posts[:3]):
                print(f"\n  --- í¬ìŠ¤íŠ¸ #{i+1} ---")
                # í´ë˜ìŠ¤ ì¶œë ¥
                classes = post.get("class", [])
                print(f"  classes: {classes}")
                print(f"  id: {post.get('id', 'none')}")

                # ì „ì²´ í…ìŠ¤íŠ¸ (ì¤„ì—¬ì„œ)
                text = post.get_text(separator=" | ", strip=True)[:200]
                print(f"  text: {text}")

                # score í›„ë³´ íƒìƒ‰
                score_selectors = [
                    ".score", ".likes", ".points", ".votes",
                    "[class*='score']", "[class*='vote']", "[class*='point']",
                    ".post_score", ".post-score", ".post_votes",
                ]
                for sel in score_selectors:
                    score_el = post.select_one(sel)
                    if score_el:
                        print(f"  ğŸ¯ SCORE [{sel}]: '{score_el.get_text(strip=True)}'")
                        print(f"     attrs: {dict(score_el.attrs)}")

                # ëŒ“ê¸€ ìˆ˜ í›„ë³´ íƒìƒ‰
                comment_selectors = [
                    ".comments", "[class*='comment']",
                    "a[href*='comments']",
                ]
                for sel in comment_selectors:
                    comment_els = post.select(sel)
                    for cel in comment_els[:2]:
                        txt = cel.get_text(strip=True)
                        if txt:
                            print(f"  ğŸ’¬ COMMENTS [{sel}]: '{txt}'")
                            print(f"     attrs: {dict(cel.attrs)}")

                # ì œëª© í›„ë³´
                title_selectors = [
                    "a.post_title", "a[class*='title']", "h2 a", "h3 a",
                    ".post_title", ".title", "p.post_title",
                ]
                for sel in title_selectors:
                    title_el = post.select_one(sel)
                    if title_el:
                        print(f"  ğŸ“Œ TITLE [{sel}]: '{title_el.get_text(strip=True)[:100]}'")
                        href = title_el.get("href", "")
                        if href:
                            print(f"     href: {href}")

                # ì‘ì„±ì
                author_selectors = [
                    "a[class*='author']", ".author", "[class*='author']",
                ]
                for sel in author_selectors:
                    auth_el = post.select_one(sel)
                    if auth_el:
                        print(f"  ğŸ‘¤ AUTHOR [{sel}]: '{auth_el.get_text(strip=True)}'")

        # â”€â”€ 3ë‹¨ê³„: ì „ì²´ HTMLì—ì„œ íŒ¨í„´ ì¶”ì¶œ â”€â”€
        print(f"\n  [ì „ì²´ HTML íŒ¨í„´ ë¶„ì„]")

        # score íŒ¨í„´
        all_score = soup.select("[class*='score']")
        print(f"  *score* class ìš”ì†Œ: {len(all_score)}ê°œ")
        for s in all_score[:3]:
            print(f"    tag={s.name}, class={s.get('class')}, text='{s.get_text(strip=True)[:50]}'")

        # vote íŒ¨í„´
        all_vote = soup.select("[class*='vote']")
        print(f"  *vote* class ìš”ì†Œ: {len(all_vote)}ê°œ")
        for v in all_vote[:3]:
            print(f"    tag={v.name}, class={v.get('class')}, text='{v.get_text(strip=True)[:50]}'")

        # comment ë§í¬
        all_comment_links = soup.select("a[href*='/comments/']")
        print(f"  comments ë§í¬: {len(all_comment_links)}ê°œ")
        for c in all_comment_links[:3]:
            print(f"    text='{c.get_text(strip=True)[:50]}', href={c.get('href','')[:80]}")

        # â”€â”€ 4ë‹¨ê³„: Raw HTML ìƒ˜í”Œ (ì²« í¬ìŠ¤íŠ¸) â”€â”€
        if found_posts:
            print(f"\n  [Raw HTML ìƒ˜í”Œ - ì²« í¬ìŠ¤íŠ¸]")
            raw = str(found_posts[0])
            # 2000ìë¡œ ì œí•œ
            if len(raw) > 2000:
                print(f"  (ì´ {len(raw)}ì, ì• 2000ìë§Œ)")
                raw = raw[:2000]
            print(raw)

        return {
            "url": base_url,
            "status": resp.status_code,
            "posts_found": len(found_posts) if found_posts else 0,
            "selector": found_selector,
            "has_score": len(all_score) > 0,
            "has_comments": len(all_comment_links) > 0,
        }

    except requests.exceptions.Timeout:
        print(f"  âŒ íƒ€ì„ì•„ì›ƒ (15ì´ˆ)")
        return None
    except requests.exceptions.ConnectionError as e:
        print(f"  âŒ ì—°ê²° ì‹¤íŒ¨: {e}")
        return None
    except Exception as e:
        print(f"  âŒ ì˜ˆì™¸: {e}")
        return None


def test_json_endpoint(base_url, subreddit=TEST_SUBREDDIT):
    """í˜¹ì‹œ JSON ì—”ë“œí¬ì¸íŠ¸ê°€ ìˆëŠ”ì§€ ì‹œë„"""
    json_urls = [
        f"{base_url}/r/{subreddit}.json",
        f"{base_url}/r/{subreddit}/hot.json",
    ]
    for url in json_urls:
        try:
            resp = requests.get(url, headers=HEADERS, timeout=10)
            ct = resp.headers.get("content-type", "")
            print(f"  JSON [{resp.status_code}] {url} (content-type: {ct})")
            if resp.status_code == 200 and "json" in ct:
                data = resp.json()
                print(f"    âœ… JSON ì‘ë‹µ! keys: {list(data.keys()) if isinstance(data, dict) else type(data)}")
                return True
        except Exception as e:
            print(f"  JSON [FAIL] {url}: {e}")
    return False


# â”€â”€ ë©”ì¸ â”€â”€

def main():
    print("=" * 60)
    print("Redlib ì¸ìŠ¤í„´ìŠ¤ HTML íŒŒì‹± í…ŒìŠ¤íŠ¸")
    print(f"í…ŒìŠ¤íŠ¸ ì„œë¸Œë ˆë”§: r/{TEST_SUBREDDIT}")
    print("=" * 60)

    results = []

    for inst in REDLIB_INSTANCES:
        print(f"\n{'#'*60}")
        print(f"# ì¸ìŠ¤í„´ìŠ¤: {inst}")
        print(f"{'#'*60}")

        # JSON ë¨¼ì € ì‹œë„
        json_ok = test_json_endpoint(inst)
        if json_ok:
            results.append({"url": inst, "method": "json", "success": True})
            time.sleep(1)
            continue

        # HTML íŒŒì‹± ì‹œë„
        result = test_html_parsing(inst)
        if result:
            results.append({**result, "method": "html", "success": True})
        else:
            results.append({"url": inst, "method": None, "success": False})

        time.sleep(2)  # ì¸ìŠ¤í„´ìŠ¤ê°„ ê°„ê²©

    # â”€â”€ ê²°ê³¼ ìš”ì•½ â”€â”€
    print("\n" + "=" * 60)
    print("ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    working = [r for r in results if r.get("success")]
    failed = [r for r in results if not r.get("success")]

    if working:
        print(f"\nâœ… ì ‘ê·¼ ê°€ëŠ¥: {len(working)}ê°œ")
        for w in working:
            method = w.get("method", "?")
            posts = w.get("posts_found", "?")
            has_score = w.get("has_score", "?")
            has_comments = w.get("has_comments", "?")
            print(f"  {w['url']}")
            print(f"    ë°©ë²•: {method} | í¬ìŠ¤íŠ¸: {posts}ê°œ | score: {has_score} | comments: {has_comments}")
    else:
        print("\nâŒ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ì ‘ê·¼ ì‹¤íŒ¨")
        print("â†’ .rss fallback ìœ ì§€ + ì†ŒìŠ¤ ê· ë“±ë°°ë¶„ìœ¼ë¡œ ì»¤ë²„")

    if failed:
        print(f"\nâŒ ì‹¤íŒ¨: {len(failed)}ê°œ")
        for f in failed:
            print(f"  {f['url']}")

    # JSONìœ¼ë¡œë„ ì €ì¥ (ë””ë²„ê¹…ìš©)
    with open("redlib_test_results.json", "w") as f:
        json.dump(results, f, indent=2, default=str)
    print(f"\nìƒì„¸ ê²°ê³¼: redlib_test_results.json")

    return 0 if working else 1


if __name__ == "__main__":
    sys.exit(main())
