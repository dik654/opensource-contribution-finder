"""
í¬ë¡¤ëŸ¬ - ë§¤ 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰, ì†ŒìŠ¤ë³„ ì£¼ê¸° ì°¨ë“± ì ìš©
  - HN + RSS: ë§¤ ì‹¤í–‰ (1ì‹œê°„ë§ˆë‹¤)
  - Reddit:    8ì‹œê°„ë§ˆë‹¤ (í•˜ë£¨ 3íšŒ)
               â†’ hot ì•Œê³ ë¦¬ì¦˜ decay ~12.5h, í”¼í¬ 3íšŒ/ì¼ (06-09, 12-14, 19-21 ET)
               â†’ 8h ê°„ê²©ì´ë©´ ëª¨ë“  í”¼í¬ì˜ ì¸ê¸°ê¸€ì„ ë†“ì¹˜ì§€ ì•ŠìŒ

Reddit ì ‘ê·¼ ìš°ì„ ìˆœìœ„:
  1. Redlib HTML íŒŒì‹± (score/ëŒ“ê¸€ ìˆ˜ í¬í•¨, ì„œë“œíŒŒí‹° í”„ë¡ íŠ¸ì—”ë“œ)
  2. PullPush API (ë¹„ê³µì‹ ì•„ì¹´ì´ë¸Œ)
  3. Reddit .json ì§ì ‘ ì ‘ê·¼
  4. Reddit .rss í”¼ë“œ (feedparser, API í‚¤ ë¶ˆí•„ìš”, score/ëŒ“ê¸€ ì—†ìŒ)
"""

import os
import re
import json
import asyncio
import aiohttp
import feedparser
from datetime import datetime, timezone, timedelta
from pathlib import Path
from bs4 import BeautifulSoup

DATA_DIR = Path(__file__).parent / "data"
POSTS_FILE = DATA_DIR / "posts.json"
STATE_FILE = DATA_DIR / "crawl_state.json"

# â”€â”€â”€ ì‹¤í–‰ ì£¼ê¸° ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Reddit: NíšŒë§ˆë‹¤ ì‹¤í–‰ (1ì‹œê°„ ê°„ê²© ê¸°ì¤€, 8 = 8ì‹œê°„ë§ˆë‹¤)
REDDIT_EVERY_N = 8

# PullPush API (Reddit ë¹„ê³µì‹ ì•„ì¹´ì´ë¸Œ - ì¸ì¦ ë¶ˆí•„ìš”)
PULLPUSH_BASE = "https://api.pullpush.io/reddit"
PULLPUSH_HEADERS = {"User-Agent": "DailyTrendBot/1.0"}

# Reddit ì§ì ‘ ì ‘ê·¼ (fallback)
REDDIT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
}

# â”€â”€â”€ Redlib ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í…ŒìŠ¤íŠ¸ ê²°ê³¼ í™•ì¸ëœ ì¸ìŠ¤í„´ìŠ¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
REDLIB_INSTANCES = [
    "https://redlib.perennialte.ch",       # âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ (57KB, 25 posts)
    "https://redlib.privacyredirect.com",
    "https://reddit.adminforge.de",
    "https://reddit.nerdvpn.de",
    "https://redlib.thebunny.zone",
    "https://safereddit.com",
    "https://redlib.catsarch.com",
    "https://redlib.r4fo.com",
    "https://redlib.4o1x5.dev",
    "https://eu.safereddit.com",
]

REDLIB_HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
}

# Redlib rate limit ë°©ì–´
REDLIB_DELAY_BETWEEN_SUBS = 3       # ì„œë¸Œë ˆë”§ ê°„ ëŒ€ê¸° (ì´ˆ)
REDLIB_DELAY_BETWEEN_INSTANCES = 2  # ì¸ìŠ¤í„´ìŠ¤ fallback ê°„ ëŒ€ê¸° (ì´ˆ)
REDLIB_TIMEOUT = 20                 # ë‹¨ì¼ ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
REDLIB_MIN_HTML_SIZE = 10000        # ì°¨ë‹¨ëœ ì‘ë‹µ íŒë³„ ê¸°ì¤€ (ë°”ì´íŠ¸)


# â”€â”€â”€ ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_state() -> dict:
    if STATE_FILE.exists():
        try:
            with open(STATE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, Exception):
            pass
    return {"run_count": 0, "last_reddit": None, "last_run": None}


def save_state(state: dict):
    DATA_DIR.mkdir(exist_ok=True)
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def should_run_reddit(state: dict) -> bool:
    return state["run_count"] % REDDIT_EVERY_N == 0


# â”€â”€â”€ ê³µí†µ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch_json(session: aiohttp.ClientSession, url: str, headers: dict = None):
    try:
        async with session.get(url, headers=headers or {}, timeout=aiohttp.ClientTimeout(total=15)) as resp:
            if resp.status == 200:
                return await resp.json()
            else:
                print(f"  [WARN] {url} â†’ HTTP {resp.status}")
    except Exception as e:
        print(f"  [WARN] {url} - {e}")
    return None


def _pick_reddit_image(d: dict) -> str:
    post_url = d.get("url", "")
    if any(post_url.endswith(ext) for ext in (".jpg", ".jpeg", ".png", ".gif", ".webp")):
        return post_url
    preview = d.get("preview", {})
    images = preview.get("images", [])
    if images:
        source = images[0].get("source", {})
        url = source.get("url", "")
        if url:
            return url
    thumb = d.get("thumbnail", "")
    if thumb.startswith("http") and thumb not in ("self", "default", "nsfw", "spoiler"):
        return thumb
    return ""


# â”€â”€â”€ Reddit ë°©ë²• 0: Redlib HTML íŒŒì‹± (ìµœìš°ì„ ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _parse_redlib_score(score_div) -> int:
    """post_score divì—ì„œ ì •í™•í•œ ìˆ«ì ì¶”ì¶œ.
    title ì†ì„±ì— '17010' ê°™ì€ ì›ë³¸ ìˆ«ìê°€ ìˆìŒ."""
    if not score_div:
        return 0
    title = score_div.get("title", "")
    if title and title.isdigit():
        return int(title)
    text = score_div.get_text(strip=True)
    m = re.match(r"([\d.]+)\s*k", text, re.IGNORECASE)
    if m:
        return int(float(m.group(1)) * 1000)
    m = re.match(r"([\d,]+)", text)
    if m:
        return int(m.group(1).replace(",", ""))
    return 0


def _parse_redlib_comments(comment_a) -> int:
    """post_comments ë§í¬ì—ì„œ ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ.
    title ì†ì„±ì— '522 comments' ê°™ì€ í…ìŠ¤íŠ¸ê°€ ìˆìŒ."""
    if not comment_a:
        return 0
    title = comment_a.get("title", "")
    m = re.search(r"(\d[\d,]*)", title)
    if m:
        return int(m.group(1).replace(",", ""))
    text = comment_a.get_text(strip=True)
    m = re.match(r"([\d.]+)\s*k", text, re.IGNORECASE)
    if m:
        return int(float(m.group(1)) * 1000)
    m = re.search(r"(\d[\d,]*)", text)
    if m:
        return int(m.group(1).replace(",", ""))
    return 0


def _parse_redlib_html(html: str, subreddit: str, base_url: str) -> list:
    """Redlib HTMLì—ì„œ í¬ìŠ¤íŠ¸ ëª©ë¡ íŒŒì‹±.

    í™•ì¸ëœ êµ¬ì¡°:
      <div class="post" id="1qw9vkj">
        <a class="post_author" href="/u/...">u/Author</a>
        <h2 class="post_title"><a href="/r/.../comments/...">ì œëª©</a></h2>
        <div class="post_score" title="17010">17.0k<span>Upvotes</span></div>
        <a class="post_comments" title="522 comments">522 comments</a>
      </div>
    """
    soup = BeautifulSoup(html, "html.parser")
    posts = []

    for post_div in soup.select("div.post"):
        try:
            post_id = post_div.get("id", "")
            if not post_id:
                continue

            title_el = post_div.select_one("h2.post_title a, a.post_title")
            if not title_el:
                continue
            title = title_el.get_text(strip=True)
            href = title_el.get("href", "")

            score_div = post_div.select_one("div.post_score, .post_score")
            score = _parse_redlib_score(score_div)

            comment_a = post_div.select_one("a.post_comments")
            comments = _parse_redlib_comments(comment_a)

            author_el = post_div.select_one("a.post_author")
            author = author_el.get_text(strip=True).replace("u/", "") if author_el else ""

            # ì¸ë„¤ì¼
            thumb_el = post_div.select_one("a.post_thumbnail")
            thumbnail = ""
            external_url = ""
            if thumb_el:
                external_url = thumb_el.get("href", "")
                img_el = thumb_el.select_one("image")
                if img_el:
                    img_href = img_el.get("href", "")
                    if img_href:
                        thumbnail = f"{base_url}{img_href}" if img_href.startswith("/") else img_href

            # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°
            body_el = post_div.select_one("div.post_body")
            hint = body_el.get_text(strip=True)[:300] if body_el else ""

            # permalink
            permalink = f"/r/{subreddit}/comments/{post_id}/"
            if href and "/comments/" in href:
                permalink = href

            posts.append({
                "id": f"reddit_{post_id}",
                "source": f"Reddit r/{subreddit}",
                "title": title,
                "url": external_url if external_url and external_url.startswith("http") else f"https://reddit.com{permalink}",
                "permalink": permalink,
                "score": score,
                "comments": comments,
                "hint": hint,
                "thumbnail": thumbnail,
                "top_comments": [],
            })

        except Exception as e:
            # ê°œë³„ í¬ìŠ¤íŠ¸ íŒŒì‹± ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì†
            continue

    return posts


async def _redlib_fetch_one(
    session: aiohttp.ClientSession,
    base_url: str,
    subreddit: str,
) -> list:
    """ë‹¨ì¼ Redlib ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ë‹¨ì¼ ì„œë¸Œë ˆë”§ HTML ê°€ì ¸ì™€ íŒŒì‹±.
    ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
    url = f"{base_url}/r/{subreddit}/hot"
    try:
        async with session.get(
            url,
            headers=REDLIB_HEADERS,
            timeout=aiohttp.ClientTimeout(total=REDLIB_TIMEOUT),
        ) as resp:
            if resp.status != 200:
                return []
            html = await resp.text()
            # ì°¨ë‹¨ëœ ì¸ìŠ¤í„´ìŠ¤ëŠ” 4-6KB ë¹ˆ í˜ì´ì§€ ë°˜í™˜
            if len(html) < REDLIB_MIN_HTML_SIZE:
                return []
            return _parse_redlib_html(html, subreddit, base_url)
    except (aiohttp.ClientError, asyncio.TimeoutError):
        return []


async def _redlib_find_working_instance(session: aiohttp.ClientSession) -> str | None:
    """ì‘ë™í•˜ëŠ” Redlib ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì°¾ì•„ ë°˜í™˜.
    todayilearned ì„œë¸Œë ˆë”§ìœ¼ë¡œ í…ŒìŠ¤íŠ¸."""
    for inst in REDLIB_INSTANCES:
        posts = await _redlib_fetch_one(session, inst, "todayilearned")
        if posts:
            return inst
        await asyncio.sleep(REDLIB_DELAY_BETWEEN_INSTANCES)
    return None


async def fetch_reddit_redlib(
    session: aiohttp.ClientSession,
    subreddits: list,
    limit_per_sub: int = 8,
) -> list:
    """Redlib HTML íŒŒì‹±ìœ¼ë¡œ ì—¬ëŸ¬ ì„œë¸Œë ˆë”§ì˜ hot í¬ìŠ¤íŠ¸ ìˆ˜ì§‘.

    ë°©ì–´ì  ì „ëµ:
    - ë¨¼ì € ì‘ë™í•˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ 1ê°œë¥¼ ì°¾ìŒ
    - í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ë¡œ ëª¨ë“  ì„œë¸Œë ˆë”§ ìˆœíšŒ
    - ì¤‘ê°„ì— ì‹¤íŒ¨í•˜ë©´ ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ë¡œ êµì²´
    - ì„œë¸Œë ˆë”§ ê°„ 3ì´ˆ ëŒ€ê¸° (rate limit ë°©ì–´)
    - ì—°ì† ì‹¤íŒ¨ 3íšŒ ì‹œ ì¡°ê¸° ì¢…ë£Œ
    """
    print("  [Redlib] ì‘ë™ ì¸ìŠ¤í„´ìŠ¤ íƒìƒ‰ ì¤‘...")
    working = await _redlib_find_working_instance(session)

    if not working:
        print("  [Redlib] âŒ ì‘ë™í•˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ")
        return []

    print(f"  [Redlib] âœ… ì‚¬ìš© ì¸ìŠ¤í„´ìŠ¤: {working}")

    all_posts = []
    success = 0
    consecutive_fails = 0
    max_consecutive_fails = 3  # ì—°ì† 3íšŒ ì‹¤íŒ¨ ì‹œ ì¸ìŠ¤í„´ìŠ¤ êµì²´ ë˜ëŠ” ì¢…ë£Œ
    remaining_instances = [i for i in REDLIB_INSTANCES if i != working]

    for i, sub in enumerate(subreddits):
        posts = await _redlib_fetch_one(session, working, sub)

        if posts:
            all_posts.extend(posts[:limit_per_sub])
            success += 1
            consecutive_fails = 0
        else:
            consecutive_fails += 1

            if consecutive_fails >= max_consecutive_fails:
                # í˜„ì¬ ì¸ìŠ¤í„´ìŠ¤ê°€ ì£½ì—ˆì„ ìˆ˜ ìˆìŒ â†’ ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ ì‹œë„
                print(f"  [Redlib] âš ï¸ ì—°ì† {consecutive_fails}íšŒ ì‹¤íŒ¨, ì¸ìŠ¤í„´ìŠ¤ êµì²´ ì‹œë„...")
                new_working = None
                for fallback in remaining_instances:
                    test_posts = await _redlib_fetch_one(session, fallback, "todayilearned")
                    if test_posts:
                        new_working = fallback
                        break
                    await asyncio.sleep(REDLIB_DELAY_BETWEEN_INSTANCES)

                if new_working:
                    print(f"  [Redlib] ğŸ”„ ìƒˆ ì¸ìŠ¤í„´ìŠ¤: {new_working}")
                    working = new_working
                    remaining_instances = [i for i in remaining_instances if i != new_working]
                    consecutive_fails = 0
                else:
                    print(f"  [Redlib] âŒ ëŒ€ì²´ ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ, ì¤‘ë‹¨ ({success}/{i+1} ì„±ê³µ)")
                    break

        # rate limit ë°©ì–´: ì„œë¸Œë ˆë”§ ê°„ ëŒ€ê¸°
        if i < len(subreddits) - 1:
            await asyncio.sleep(REDLIB_DELAY_BETWEEN_SUBS)

    print(f"  Redlib: {len(subreddits)}ê°œ ì„œë¸Œë ˆë”§ ì¤‘ {success}ê°œ ì„±ê³µ, {len(all_posts)}ê°œ ê¸€")
    return all_posts


# â”€â”€â”€ Reddit ë°©ë²• 1: PullPush API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch_reddit_pullpush(session: aiohttp.ClientSession, subreddit: str, limit: int = 10):
    after_epoch = int((datetime.now(timezone.utc) - timedelta(hours=48)).timestamp())
    url = (
        f"{PULLPUSH_BASE}/search/submission/"
        f"?subreddit={subreddit}&sort=desc&sort_type=score"
        f"&size={limit}&after={after_epoch}"
    )
    data = await fetch_json(session, url, PULLPUSH_HEADERS)
    if not data or "data" not in data:
        return []
    posts = []
    for d in data["data"]:
        if d.get("stickied") or d.get("removed_by_category"):
            continue
        permalink = d.get("permalink", f"/r/{subreddit}/comments/{d.get('id', '')}/")
        posts.append({
            "id": f"reddit_{d.get('id', '')}",
            "source": f"Reddit r/{subreddit}",
            "title": d.get("title", ""),
            "url": f"https://reddit.com{permalink}",
            "permalink": permalink,
            "score": d.get("score", 0),
            "comments": d.get("num_comments", 0),
            "hint": (d.get("selftext", "") or "")[:300],
            "thumbnail": _pick_reddit_image(d),
            "top_comments": [],
        })
    return posts


# â”€â”€â”€ Reddit ë°©ë²• 2: .json ì§ì ‘ ì ‘ê·¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch_reddit_direct(session: aiohttp.ClientSession, subreddit: str, limit: int = 10):
    endpoints = [
        f"https://old.reddit.com/r/{subreddit}/hot.json?limit={limit}&raw_json=1",
        f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}&raw_json=1",
    ]
    data = None
    for ep in endpoints:
        data = await fetch_json(session, ep, REDDIT_HEADERS)
        if data:
            break
        await asyncio.sleep(0.5)
    if not data:
        return []
    posts = []
    for child in data.get("data", {}).get("children", []):
        d = child.get("data", {})
        if d.get("stickied"):
            continue
        posts.append({
            "id": f"reddit_{d.get('id', '')}",
            "source": f"Reddit r/{subreddit}",
            "title": d.get("title", ""),
            "url": f"https://reddit.com{d.get('permalink', '')}",
            "permalink": d.get("permalink", ""),
            "score": d.get("score", 0),
            "comments": d.get("num_comments", 0),
            "hint": (d.get("selftext", "") or "")[:300],
            "thumbnail": _pick_reddit_image(d),
            "top_comments": [],
        })
    return posts


# â”€â”€â”€ Reddit ë°©ë²• 3: .rss í”¼ë“œ (ìµœì¢… fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_reddit_rss(subreddit: str, limit: int = 10) -> list:
    url = f"https://www.reddit.com/r/{subreddit}/.rss?limit={limit}"
    try:
        feed = feedparser.parse(url)
        if feed.bozo and not feed.entries:
            return []
    except Exception as e:
        print(f"  [WARN] Reddit RSS r/{subreddit} - {e}")
        return []

    posts = []
    for entry in feed.entries[:limit]:
        link = entry.get("link", "")
        title = entry.get("title", "")
        entry_id = entry.get("id", "")
        reddit_id = entry_id.split("_")[-1] if "_" in entry_id else str(hash(link) % 10**10)
        permalink = ""
        if "reddit.com" in link:
            permalink = link.split("reddit.com")[-1]
        thumb = ""
        if hasattr(entry, "content"):
            for c in entry.content:
                html = c.get("value", "")
                img_match = re.search(r'<img\s+src="([^"]+)"', html)
                if img_match:
                    thumb = img_match.group(1)
                    break
        if not thumb:
            media_thumb = entry.get("media_thumbnail", [])
            if media_thumb and isinstance(media_thumb, list):
                thumb = media_thumb[0].get("url", "")
        summary = entry.get("summary", "") or ""
        hint = re.sub(r'<[^>]+>', '', summary)[:300]
        posts.append({
            "id": f"reddit_{reddit_id}",
            "source": f"Reddit r/{subreddit}",
            "title": title,
            "url": link,
            "permalink": permalink,
            "score": 0,
            "comments": 0,
            "hint": hint,
            "thumbnail": thumb,
            "top_comments": [],
        })
    return posts


def fetch_reddit_rss_multi(subreddits: list, limit_per_sub: int = 8) -> list:
    all_posts = []
    chunk_size = 6

    for i in range(0, len(subreddits), chunk_size):
        chunk = subreddits[i:i + chunk_size]
        combined = "+".join(chunk)
        url = f"https://www.reddit.com/r/{combined}/hot/.rss?limit={limit_per_sub * len(chunk)}"
        try:
            feed = feedparser.parse(url)
            if feed.bozo and not feed.entries:
                for sub in chunk:
                    all_posts.extend(fetch_reddit_rss(sub, limit=limit_per_sub))
                continue
        except Exception:
            for sub in chunk:
                all_posts.extend(fetch_reddit_rss(sub, limit=limit_per_sub))
            continue

        for entry in feed.entries:
            link = entry.get("link", "")
            title = entry.get("title", "")
            entry_id = entry.get("id", "")
            reddit_id = entry_id.split("_")[-1] if "_" in entry_id else str(hash(link) % 10**10)
            permalink = ""
            if "reddit.com" in link:
                permalink = link.split("reddit.com")[-1]
            source_sub = "unknown"
            if permalink:
                parts = permalink.split("/")
                if len(parts) >= 3 and parts[1] == "r":
                    source_sub = parts[2]
            thumb = ""
            if hasattr(entry, "content"):
                for c in entry.content:
                    html = c.get("value", "")
                    img_match = re.search(r'<img\s+src="([^"]+)"', html)
                    if img_match:
                        thumb = img_match.group(1)
                        break
            if not thumb:
                media_thumb = entry.get("media_thumbnail", [])
                if media_thumb and isinstance(media_thumb, list):
                    thumb = media_thumb[0].get("url", "")
            summary = entry.get("summary", "") or ""
            hint = re.sub(r'<[^>]+>', '', summary)[:300]
            all_posts.append({
                "id": f"reddit_{reddit_id}",
                "source": f"Reddit r/{source_sub}",
                "title": title,
                "url": link,
                "permalink": permalink,
                "score": 0,
                "comments": 0,
                "hint": hint,
                "thumbnail": thumb,
                "top_comments": [],
            })

    return all_posts


# â”€â”€â”€ Reddit í†µí•© + ëŒ“ê¸€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch_reddit(session: aiohttp.ClientSession, subreddit: str, limit: int = 10, use_pullpush: bool = True):
    if use_pullpush:
        posts = await fetch_reddit_pullpush(session, subreddit, limit)
        if posts:
            return posts
    posts = await fetch_reddit_direct(session, subreddit, limit)
    if posts:
        return posts
    return fetch_reddit_rss(subreddit, limit)


async def fetch_reddit_comments_pullpush(session: aiohttp.ClientSession, submission_id: str, limit: int = 3):
    url = (
        f"{PULLPUSH_BASE}/search/comment/"
        f"?link_id={submission_id}&sort=desc&sort_type=score&size={limit}"
    )
    data = await fetch_json(session, url, PULLPUSH_HEADERS)
    if not data or "data" not in data:
        return []
    comments = []
    for c in data["data"]:
        body = (c.get("body", "") or "")[:200]
        if not body or c.get("stickied") or body == "[deleted]" or body == "[removed]":
            continue
        comments.append({"author": c.get("author", ""), "body": body, "score": c.get("score", 0)})
    comments.sort(key=lambda x: x["score"], reverse=True)
    return comments[:3]


async def fetch_reddit_comments_direct(session: aiohttp.ClientSession, permalink: str, limit: int = 3):
    endpoints = [
        f"https://old.reddit.com{permalink}.json?sort=top&limit={limit}&raw_json=1",
        f"https://www.reddit.com{permalink}.json?sort=top&limit={limit}&raw_json=1",
    ]
    data = None
    for ep in endpoints:
        data = await fetch_json(session, ep, REDDIT_HEADERS)
        if data:
            break
        await asyncio.sleep(0.5)
    if not data or len(data) < 2:
        return []
    comments = []
    for child in data[1].get("data", {}).get("children", []):
        if child.get("kind") != "t1":
            continue
        c = child["data"]
        body = (c.get("body", "") or "")[:200]
        if not body or c.get("stickied"):
            continue
        comments.append({"author": c.get("author", ""), "body": body, "score": c.get("score", 0)})
    comments.sort(key=lambda x: x["score"], reverse=True)
    return comments[:3]


# â”€â”€â”€ HN + ì¼ë°˜ RSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch_hackernews(session: aiohttp.ClientSession, limit: int = 15):
    ids = await fetch_json(session, "https://hacker-news.firebaseio.com/v0/topstories.json")
    if not ids:
        return []
    posts = []
    for item_id in ids[:limit]:
        item = await fetch_json(session, f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json")
        if not item or item.get("type") != "story":
            continue
        posts.append({
            "id": f"hn_{item_id}",
            "source": "Hacker News",
            "title": item.get("title", ""),
            "url": item.get("url", f"https://news.ycombinator.com/item?id={item_id}"),
            "score": item.get("score", 0),
            "comments": item.get("descendants", 0),
            "hint": "",
            "thumbnail": "",
        })
    return posts


def fetch_rss(feed_url: str, source_name: str, limit: int = 10):
    try:
        feed = feedparser.parse(feed_url)
        posts = []
        for entry in feed.entries[:limit]:
            link = entry.get("link", "")
            thumb = ""
            media_thumb = entry.get("media_thumbnail", [])
            if media_thumb and isinstance(media_thumb, list):
                thumb = media_thumb[0].get("url", "")
            if not thumb:
                media_content = entry.get("media_content", [])
                if media_content and isinstance(media_content, list):
                    for mc in media_content:
                        if mc.get("medium") == "image" or (mc.get("type", "").startswith("image")):
                            thumb = mc.get("url", "")
                            break
            if not thumb:
                enclosures = entry.get("enclosures", [])
                if enclosures:
                    for enc in enclosures:
                        if enc.get("type", "").startswith("image"):
                            thumb = enc.get("href", "") or enc.get("url", "")
                            break
            posts.append({
                "id": f"rss_{hash(link) % 10**10}",
                "source": source_name,
                "title": entry.get("title", ""),
                "url": link,
                "score": 0,
                "comments": 0,
                "hint": (entry.get("summary", "") or "")[:300],
                "thumbnail": thumb,
            })
        return posts
    except Exception as e:
        print(f"  [WARN] RSS {source_name} - {e}")
        return []


# â”€â”€â”€ ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

REDDIT_SUBS = [
    "todayilearned", "science", "worldnews", "Futurology",
    "LifeProTips", "movies", "television", "food",
    "UpliftingNews", "explainlikeimfive",
    "Coffee", "tea", "whiskey", "CraftBeer", "fragrance",
    "puzzles", "DIY", "Breadit", "Baking", "knitting",
    "Embroidery", "cocktails", "minipainting", "modelmakers",
    "dadjokes", "tifu", "antiwork", "AmItheAsshole",
    "NoStupidQuestions",
]

RSS_SOURCES = [
    ("https://feeds.bbci.co.uk/news/world/rss.xml", "BBC World"),
    ("https://feeds.reuters.com/reuters/topNews", "Reuters"),
    ("https://www.theverge.com/rss/index.xml", "The Verge"),
    ("https://feeds.npr.org/1001/rss.xml", "NPR"),
    ("https://www.nature.com/nature.rss", "Nature"),
    ("https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml", "NYT"),
]


async def collect_reddit(session: aiohttp.ClientSession) -> list:
    """Reddit ì „ì²´ ìˆ˜ì§‘: Redlib â†’ PullPush â†’ .json â†’ .rss ìˆœì„œë¡œ ì‹œë„"""

    # â”€â”€ 1ë‹¨ê³„: Redlib HTML íŒŒì‹± (score + ëŒ“ê¸€ ìˆ˜ í¬í•¨) â”€â”€
    print("  [1/4] Redlib HTML íŒŒì‹± ì‹œë„...")
    redlib_posts = await fetch_reddit_redlib(session, REDDIT_SUBS, limit_per_sub=8)
    if redlib_posts:
        print(f"  [Redlib] ìˆ˜ì§‘ ì™„ë£Œ: {len(redlib_posts)}ê°œ ê¸€ (score/ëŒ“ê¸€ í¬í•¨)")
        # Redlibì€ ëŒ“ê¸€ ë³¸ë¬¸ì„ ëª» ê°€ì ¸ì˜¤ë¯€ë¡œ, score ë†’ì€ ê¸€ì— ëŒ€í•´
        # PullPush/ì§ì ‘ì ‘ê·¼ìœ¼ë¡œ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œë„
        await _enrich_comments(session, redlib_posts)
        return redlib_posts

    # â”€â”€ 2ë‹¨ê³„: PullPush API â”€â”€
    print("  [2/4] PullPush API ì‹œë„...")
    use_pullpush = False
    api_available = False
    test = await fetch_reddit_pullpush(session, "todayilearned", limit=2)
    if test:
        use_pullpush = True
        api_available = True
        print("  [OK] PullPush API ì‚¬ìš©")
    else:
        # â”€â”€ 3ë‹¨ê³„: .json ì§ì ‘ ì ‘ê·¼ â”€â”€
        print("  [3/4] Reddit .json ì§ì ‘ ì ‘ê·¼ ì‹œë„...")
        test2 = await fetch_reddit_direct(session, "todayilearned", limit=2)
        if test2:
            api_available = True
            print("  [OK] Reddit .json ì§ì ‘ ì ‘ê·¼ ì‚¬ìš©")
        else:
            # â”€â”€ 4ë‹¨ê³„: .rss í”¼ë“œ â”€â”€
            print("  [4/4] Reddit .rss í”¼ë“œ ì‹œë„...")
            test3 = fetch_reddit_rss("todayilearned", limit=2)
            if test3:
                print("  [OK] Reddit .rss í”¼ë“œ ì‚¬ìš© (score/ëŒ“ê¸€ ìˆ˜ ì—†ìŒ)")
            else:
                print("  [WARN] Reddit ì ‘ê·¼ ì™„ì „ ë¶ˆê°€ - Reddit ê±´ë„ˆëœ€")
                return []

    all_posts = []
    reddit_posts = []
    reddit_ok = 0

    if api_available:
        for sub in REDDIT_SUBS:
            posts = await fetch_reddit(session, sub, limit=8, use_pullpush=use_pullpush)
            if posts:
                reddit_ok += 1
            all_posts.extend(posts)
            for p in posts:
                if p.get("permalink") or p.get("id"):
                    reddit_posts.append(p)
            await asyncio.sleep(2 if use_pullpush else 3)
    else:
        # RSS ì¼ê´„ ìˆ˜ì§‘
        rss_posts = fetch_reddit_rss_multi(REDDIT_SUBS, limit_per_sub=8)
        all_posts.extend(rss_posts)
        reddit_ok = len(set(p["source"] for p in rss_posts)) if rss_posts else 0

    print(f"  Reddit: {len(REDDIT_SUBS)}ê°œ ì„œë¸Œë ˆë”§ ì¤‘ {reddit_ok}ê°œ ì„±ê³µ, {len(all_posts)}ê°œ ê¸€")

    # ëŒ“ê¸€ ìˆ˜ì§‘ (API ì‚¬ìš© ê°€ëŠ¥ + score ìˆëŠ” ê²½ìš°ë§Œ)
    if reddit_posts and api_available:
        await _enrich_comments_api(session, reddit_posts, use_pullpush)

    return all_posts


async def _enrich_comments(session: aiohttp.ClientSession, posts: list):
    """Redlibìœ¼ë¡œ ê°€ì ¸ì˜¨ í¬ìŠ¤íŠ¸ì˜ ìƒìœ„ ëŒ“ê¸€ì„ PullPush/.jsonìœ¼ë¡œ ë³´ê°•."""
    # score ë†’ì€ ìƒìœ„ 10ê°œë§Œ
    scored = sorted(posts, key=lambda x: x.get("score", 0), reverse=True)
    top = scored[:30]

    # PullPush ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸
    test = await fetch_reddit_pullpush(session, "todayilearned", limit=1)
    use_pullpush = bool(test)

    if not use_pullpush:
        # .json ì§ì ‘ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        test2 = await fetch_reddit_direct(session, "todayilearned", limit=1)
        if not test2:
            print("  [ëŒ“ê¸€] API ì ‘ê·¼ ë¶ˆê°€, ëŒ“ê¸€ ìˆ˜ì§‘ ê±´ë„ˆëœ€")
            return

    enriched = 0
    for p in top:
        try:
            if use_pullpush:
                sub_id = p["id"].replace("reddit_", "")
                comments = await fetch_reddit_comments_pullpush(session, sub_id)
            else:
                comments = await fetch_reddit_comments_direct(session, p["permalink"])
            if comments:
                p["top_comments"] = comments
                enriched += 1
        except Exception:
            pass
        await asyncio.sleep(2)

    if enriched:
        print(f"  [ëŒ“ê¸€] {enriched}/{len(top)}ê°œ ê¸€ì— ëŒ“ê¸€ ì¶”ê°€")


async def _enrich_comments_api(session: aiohttp.ClientSession, reddit_posts: list, use_pullpush: bool):
    """PullPush/.json APIë¡œ ê°€ì ¸ì˜¨ í¬ìŠ¤íŠ¸ì˜ ëŒ“ê¸€ ë³´ê°•."""
    reddit_posts.sort(key=lambda x: x["score"], reverse=True)
    top_reddit = reddit_posts[:30]
    print(f"  ëŒ“ê¸€ ìˆ˜ì§‘: ìƒìœ„ {len(top_reddit)}ê°œ Reddit ê¸€")
    for p in top_reddit:
        if use_pullpush:
            sub_id = p["id"].replace("reddit_", "")
            comments = await fetch_reddit_comments_pullpush(session, sub_id)
        else:
            comments = await fetch_reddit_comments_direct(session, p["permalink"])
        p["top_comments"] = comments
        await asyncio.sleep(2)


async def collect_fast(session: aiohttp.ClientSession) -> list:
    all_posts = []
    hn_posts = await fetch_hackernews(session, limit=15)
    all_posts.extend(hn_posts)
    print(f"  HN: {len(hn_posts)}ê°œ")
    rss_count = 0
    for url, name in RSS_SOURCES:
        posts = fetch_rss(url, name, limit=8)
        all_posts.extend(posts)
        rss_count += len(posts)
    print(f"  RSS: {rss_count}ê°œ ({len(RSS_SOURCES)}ê°œ í”¼ë“œ)")
    return all_posts


async def collect_all(run_reddit: bool = True):
    async with aiohttp.ClientSession() as session:
        all_posts = []
        fast_posts = await collect_fast(session)
        all_posts.extend(fast_posts)
        if run_reddit:
            reddit_posts = await collect_reddit(session)
            all_posts.extend(reddit_posts)
        else:
            print("  Reddit: ì´ë²ˆ ì‹¤í–‰ ê±´ë„ˆëœ€")
    return all_posts


# â”€â”€â”€ ë°ì´í„° ëˆ„ì  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_existing() -> dict:
    if POSTS_FILE.exists():
        try:
            with open(POSTS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, Exception):
            pass
    return {"posts": {}, "last_crawl": None}


def merge_posts(existing: dict, new_posts: list) -> dict:
    posts = existing.get("posts", {})
    for p in new_posts:
        pid = p["id"]
        if pid in posts:
            posts[pid]["score"] = max(posts[pid].get("score", 0), p["score"])
            posts[pid]["comments"] = max(posts[pid].get("comments", 0), p["comments"])
            posts[pid]["seen_count"] = posts[pid].get("seen_count", 1) + 1
            if not posts[pid].get("thumbnail") and p.get("thumbnail"):
                posts[pid]["thumbnail"] = p["thumbnail"]
            new_comments = p.get("top_comments", [])
            if new_comments:
                existing_comments = posts[pid].get("top_comments", [])
                all_comments = {c["body"][:50]: c for c in existing_comments + new_comments}
                merged_comments = sorted(all_comments.values(), key=lambda x: x["score"], reverse=True)
                posts[pid]["top_comments"] = merged_comments[:3]
        else:
            p["seen_count"] = 1
            p["first_seen"] = datetime.now(timezone.utc).isoformat()
            posts[pid] = p
    existing["posts"] = posts
    existing["last_crawl"] = datetime.now(timezone.utc).isoformat()
    return existing


def save_data(data: dict):
    DATA_DIR.mkdir(exist_ok=True)
    with open(POSTS_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main():
    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst).strftime("%Y-%m-%d %H:%M KST")
    state = load_state()
    run_count = state["run_count"]
    run_reddit = should_run_reddit(state)
    sources_label = "HN+RSS+Reddit" if run_reddit else "HN+RSS"
    print(f"[í¬ë¡¤ë§ #{run_count}] {now} ({sources_label})")

    new_posts = await collect_all(run_reddit=run_reddit)
    print(f"  ìˆ˜ì§‘ í•©ê³„: {len(new_posts)}ê°œ")

    existing = load_existing()
    before = len(existing.get("posts", {}))
    merged = merge_posts(existing, new_posts)
    after = len(merged["posts"])
    print(f"  ëˆ„ì : {before} â†’ {after}ê°œ (ì‹ ê·œ {after - before}ê°œ)")

    save_data(merged)
    print(f"  ì €ì¥ ì™„ë£Œ: {POSTS_FILE}")

    state["run_count"] = run_count + 1
    state["last_run"] = datetime.now(timezone.utc).isoformat()
    if run_reddit:
        state["last_reddit"] = datetime.now(timezone.utc).isoformat()
    save_state(state)

    next_reddit = REDDIT_EVERY_N - ((run_count + 1) % REDDIT_EVERY_N)
    if next_reddit == REDDIT_EVERY_N:
        next_reddit = 0
    print(f"  ë‹¤ìŒ Reddit í¬ë¡¤ë§: {'ì§€ê¸ˆ ì™„ë£Œ' if run_reddit else f'{next_reddit}íšŒ í›„'}")


if __name__ == "__main__":
    asyncio.run(main())
